{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c04455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import Callable, Tuple, List, Union, Dict\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchmetrics\n",
    "\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.utils import standardize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "from botorch.acquisition import UpperConfidenceBound, ExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4475b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentSetting2(BaseModel):\n",
    "    \"\"\"Dataset from which embeddings are sourced from\"\"\"\n",
    "    dataset: str\n",
    "    \"\"\"subset of the dataset to be used\"\"\"\n",
    "    subset: str\n",
    "    \"\"\"Number of initial datapoints (to build the models)\"\"\"\n",
    "    n_init: int\n",
    "    \"\"\"Number of total datapoints evaluated\"\"\"\n",
    "    n_total: int\n",
    "    \"\"\"Standardization flag for design matrix\"\"\"\n",
    "    standardize:bool\n",
    "    \"\"\"Model (type) from which the embedding vectors are sourced from\"\"\"\n",
    "    model: str\n",
    "    \"\"\"Layer (ID) of the neural network to load embeddings from\"\"\"\n",
    "    layer: int \n",
    "    \"\"\"Level of training (0: embryonic - 4: converged)\"\"\"\n",
    "    epochs: int\n",
    "    \"\"\"Similarity measure\"\"\"\n",
    "    metric: str\n",
    "    \"\"\"Objective function\"\"\"\n",
    "    objective: str\n",
    "    \"\"\"Acquisition function\"\"\"\n",
    "    acquisition: str\n",
    "    \"\"\"Modulus of bound value of optimization problem\"\"\"\n",
    "    bound: float\n",
    "    \"\"\"Beta value for UCB\"\"\"\n",
    "    beta: float\n",
    "    \"\"\"Dimension reduction technique\"\"\"\n",
    "    dim_red: str\n",
    "    \"\"\"Effective dimension (in case of dimension reduction)\"\"\"\n",
    "    dim_eff: int\n",
    "    \n",
    "class Experiment2():\n",
    "    def __init__(self, config:ExperimentSetting2):\n",
    "        self.dataset     = config.dataset\n",
    "        self.subset      = config.subset\n",
    "        self.n_init      = config.n_init\n",
    "        self.n_total     = config.n_total\n",
    "        self.standardize = config.standardize\n",
    "        self.model       = config.model\n",
    "        self.layer       = config.layer\n",
    "        self.epochs      = config.epochs\n",
    "        self.metric      = config.metric\n",
    "        self.objective   = config.objective\n",
    "        self.acquisition = config.acquisition\n",
    "        self.bound       = config.bound\n",
    "        self.beta        = config.beta\n",
    "        self.dim_red     = config.dim_red\n",
    "        self.dim_eff     = config.dim_eff\n",
    "        \n",
    "        # Check Inputs\n",
    "        assert self.n_init > 0,  'Input `n_init` must be positive.'\n",
    "        assert self.n_total > 0, 'Input `n_total` must be positive.'\n",
    "        assert self.n_total > self.n_init, f'Input `n_total`={self.n_total} must be larger than `n_init`={self.n_init}.'\n",
    "        assert self.bound > 0, \"(Modulus of componentwise) bound must be positive float.\"\n",
    "        assert self.metric in ['cosine', 'l2', 'l1'], \"Unknown `metric`\"\n",
    "        assert self.objective in ['mu_tr', 'q935_tr', 'sig_tr', 'mu+sig_tr', 'conv1_tr', 'conv2_tr', 'conv3_tr', 'conv4_tr', 'conv1+label_tr', 'conv2+label_tr', 'conv3+label_tr', 'conv4+label_tr'], \"Unknown `objectie`\"\n",
    "        assert self.acquisition in ['ucb', 'ei'], \"Acquisition function must be eitgher `ucb` or `ei`.\"\n",
    "        assert self.epochs in [0, 5, 20, 99], \"Layer id not available. Pick from {0, 5, 20, 99}.\"\n",
    "        assert (self.dim_red is None) | (self.dim_red in ['pca', 'svd']), \"Dimension reduction technique\"\n",
    "        \n",
    "        # Set metric\n",
    "        if(self.metric=='cosine'):\n",
    "            self.dist = nn.CosineSimilarity()\n",
    "        elif(self.metric=='l1'):\n",
    "            self.dist = lambda a,b: lp(a,b,p=1)\n",
    "        elif(self.metric=='l2'):\n",
    "            self.dist = lambda a,b: lp(a,b,p=2)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only `cosine` (similiarity), `l1` and `l2` implemented.\")\n",
    "            \n",
    "        # Load data\n",
    "        # - load tables\n",
    "        self.df_f_tr = pd.read_csv(f'./tables/df_{self.dataset}_{self.subset}.csv')\n",
    "        self.df_f_te = pd.read_csv(f'./tables/df_{self.dataset}_{self.subset}.csv')\n",
    "        \n",
    "        # - subset data\n",
    "        #self.df_f_tr = self.df_f_tr[0:500]\n",
    "        #self.df_f_te = self.df_f_te[0:500]\n",
    "        \n",
    "        # - emb\n",
    "        X_tr, y_tr, X_te, y_te = getEmbXy(self.epochs, self.layer)\n",
    "        # - store raw data\n",
    "        self.X_tr = X_tr\n",
    "        self.X_te = X_te\n",
    "        \n",
    "        # - convert\n",
    "        X_tr = torch.tensor(X_tr).type(torch.float64)\n",
    "        X_te = torch.tensor(X_te).type(torch.float64)\n",
    "        \n",
    "        # - siubset\n",
    "        self.X_tr = X_tr\n",
    "        self.X_te = X_te\n",
    "\n",
    "        # - load raw data\n",
    "        X_tr_raw, _, X_te_raw, _ = getEmbXy(-1, self.layer, True)\n",
    "        self.X_tr_raw = X_tr_raw\n",
    "        self.X_te_raw = X_te_raw\n",
    "        \n",
    "        # convert data\n",
    "        train_X = torch.tensor(X_tr, dtype=torch.float64)\n",
    "        train_Y = torch.tensor(self.df_f_tr[self.objective])\n",
    "        train_Y = train_Y.reshape(len(train_Y), -1)\n",
    "\n",
    "        # dimension reduction\n",
    "        if(self.dim_red=='pca'):\n",
    "            if(self.dim_eff==-1):\n",
    "                U, S, V  = torch.pca_lowrank(train_X) \n",
    "            else:\n",
    "                U, S, V  = torch.pca_lowrank(train_X, q=self.dim_eff)\n",
    "            # store\n",
    "            self.U = U\n",
    "            self.V = V\n",
    "            self.S = S\n",
    "            \n",
    "            # projection\n",
    "            proj_train_X  = torch.matmul(train_X, V)        # d=6\n",
    "            appr          = torch.matmul(proj_train_X, V.T)\n",
    "            \n",
    "            # - store raw data's projection\n",
    "            self.X_tr_proj = torch.matmul(self.X_tr, V)\n",
    "            self.X_te_proj = torch.matmul(self.X_te, V)\n",
    "            \n",
    "            # - assess quality\n",
    "            total_var = torch.var(train_X)\n",
    "            total_var_approx = torch.var(appr)\n",
    "            cum_perc = (total_var_approx / total_var) * 100\n",
    "            print(f'Dim. reduction via {self.dim_red.upper()} from d={train_X.shape[1]} to d_eff={proj_train_X.shape[1]}.\\nMaintaining {cum_perc:.2f}% of variation.')\n",
    "        else:\n",
    "            proj_train_X = train_X\n",
    "\n",
    "        # - standardize\n",
    "        if(self.standardize):\n",
    "            print('Min max')\n",
    "            # min max\n",
    "            self.train_X = min_max(proj_train_X)\n",
    "            \n",
    "            print('Standardize!')\n",
    "            #self.train_X = standardize(self.train_X)\n",
    "            self.train_Y = standardize(train_Y)\n",
    "            #self.train_Y = train_Y\n",
    "            \n",
    "           \n",
    "\n",
    "        # cast to float64\n",
    "        self.train_X = self.train_X.type(torch.float64)\n",
    "        self.train_Y = self.train_Y.type(torch.float64)\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def fit(self, n_max:int=1000, rndVal:int=5892, verbose:bool=False) -> None:\n",
    "        # replicable experiment\n",
    "        np.random.seed(rndVal)\n",
    "        rndIndex = np.random.choice(range(len(self.train_X)), n_max, False)\n",
    "        \n",
    "        # subset data\n",
    "        X = self.train_X[rndIndex,]\n",
    "        Y = self.train_Y[rndIndex,]\n",
    "        \n",
    "        # Gaussian Process\n",
    "        self.gp  = SingleTaskGP(X, Y)\n",
    "        self.mll = ExactMarginalLogLikelihood(self.gp.likelihood, self.gp)\n",
    "        \n",
    "        # - fit\n",
    "        fit_gpytorch_mll(self.mll)\n",
    "        \n",
    "        if(verbose):\n",
    "            print(\"Fitting of GP complete.\")\n",
    "        \n",
    "        # Acquisition function\n",
    "        if(self.acquisition=='ucb'):\n",
    "            self.acq_fun = UpperConfidenceBound(self.gp, beta=self.beta)\n",
    "        else:\n",
    "            raise NotImplementedError('Only `ucb` implemented for acquisition function.')\n",
    "        \n",
    "        # Bounds\n",
    "        self.bounds = torch.stack([(-1.) * self.bound * torch.ones(X.shape[1]), \n",
    "                                   self.bound * torch.ones(X.shape[1])])\n",
    "        \n",
    "        #return\n",
    "        if(verbose):\n",
    "            print(\"Bounds and acquisition defined.\")\n",
    "        \n",
    "        # AFO\n",
    "        x_cand, acq_value = optimize_acqf(\n",
    "            self.acq_fun, bounds=self.bounds, q=1, num_restarts=10, raw_samples=50,\n",
    "        )\n",
    "        \n",
    "        if(verbose):\n",
    "            print(\"Acquisition function.\")\n",
    "        \n",
    "        # register candidate \n",
    "        self.x_cand = x_cand.type(torch.float64)\n",
    "        self.acq_value = acq_value\n",
    "        \n",
    "        # flag\n",
    "        self.optimized=True\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def getXOpt(self, asImage:bool=False, mode:str='train', method='kNN') -> torch.tensor:\n",
    "        '''\n",
    "        Return the optimal candidate\n",
    "        '''\n",
    "        assert self.optimized, \"Launch `.fit()` before requesting the optimum candidate!\"\n",
    "        assert mode.lower() in ['train', 'test'], \"`mode` must be either `train` or `test` for respective lookup of the best image.\"\n",
    "        \n",
    "        if(method.lower()!='knn'):\n",
    "            raise NotImplementedError(\"Did not implement any of those methods to translate embeddinng vector to image except nearest-neighbor lookup (`1NN`)!\")\n",
    "        \n",
    "        if(asImage):\n",
    "            # closest index\n",
    "            if(method.lower()=='knn'):\n",
    "                # distance matrix: d_proj -> d_embedding\n",
    "                v_cand = self.x_cand @ self.V.T\n",
    "                if(mode=='train'):\n",
    "                    self.D = self.dist(v_cand, self.X_tr)\n",
    "                else:\n",
    "                    self.D = self.dist(v_cand, self.X_te)\n",
    "                \n",
    "                # index\n",
    "                if(self.metric=='cosine'):\n",
    "                    self.i_star = self.D.argmax()\n",
    "                else:\n",
    "                    self.i_star = self.D.argmin()\n",
    "                \n",
    "                # look up clo\n",
    "                if(mode!='train'):\n",
    "                    return self.X_te_raw[self.i_star]\n",
    "                \n",
    "                return self.X_tr_raw[self.i_star]\n",
    "                \n",
    "        \n",
    "        return self.x_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "def540d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1890086/730410305.py:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_X = torch.tensor(X_tr, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim. reduction via PCA from d=256 to d_eff=6.\n",
      "Maintaining 76.93% of variation.\n",
      "Min max\n",
      "Standardize!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/eagle/candle_aesp/siebenschuh/envs_/diff/lib/python3.10/site-packages/botorch/models/utils/assorted.py:201: InputDataWarning: Input data is not standardized. Please consider scaling the input to zero mean and unit variance.\n",
      "  warnings.warn(msg, InputDataWarning)\n"
     ]
    }
   ],
   "source": [
    "# Setup of Experiment\n",
    "# - load YAML\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# - set setting\n",
    "setting = ExperimentSetting2(**config)\n",
    "\n",
    "# - define experiment \n",
    "exp2 = Experiment2(setting)\n",
    "\n",
    "# - run experiment\n",
    "exp2.fit(n_max=500)\n",
    "img = exp2.getXOpt(asImage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ec570de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f39a329bfa0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff",
   "language": "python",
   "name": "diff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
